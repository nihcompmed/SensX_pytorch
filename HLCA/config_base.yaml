experiment:
  input_dir: "data/"
  output_dir: "analysis_results/"
  model_path: "./saved_models/"
  device: "cuda"

data_constraints:
  # Paths to numpy arrays defining the global hypercube
  global_lower_path: "global_lower.npy"
  global_upper_path: "global_upper.npy"
  
  # Path to numpy array containing indices of features to perturb
  # Set to null to perturb ALL features
  perturb_features_path: null
  # perturb_features_path: "constraints/active_indices.npy"

stability_analysis:
  # The list of delta values to scan in Step 1
  deltas: [0.0005, 0.001, 0.005, 0.01, 0.02, 0.05, 0.10, 0.15, 0.25, 0.40, 0.60, 0.85, 1.0]
  n_samples: 1000       # Number of random samples per delta (n_s)
  batch_size: 64        # Inference batch size for sampling

optimal_delta_search:
  tau_a: 0.10           # Max allowed variation (Max-Min) in the tail

sensitivity_analysis:
  # Options: "feature_batching" (Default, for Images) or "input_batching" (For Tabular/Many inputs)
  method: "feature_batching"
  n_trajectories: 50   # Number of Morris trajectories (n_w)
  batch_size: 4096        # Inference batch size for trajectory steps (vectorized chunks)
